<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI For Ancient Languages</title>
    <link>/</link>
    <description>Recent content on AI For Ancient Languages</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Will Fitzgerald</copyright>
    <lastBuildDate>Wed, 23 Apr 2025 12:52:35 -0400</lastBuildDate>
    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Word Vectors Oversimplified</title>
      <link>/posts/word-vectors-oversimplified/</link>
      <pubDate>Wed, 23 Apr 2025 12:52:35 -0400</pubDate>
      <guid>/posts/word-vectors-oversimplified/</guid>
      <description>&lt;p&gt;&lt;em&gt;This is a &lt;a href=&#34;https://willwhim.wpengine.com/2015/05/12/word-vectors-machine-learning-oversimplified/&#34;&gt;reposting of an article&lt;/a&gt; I wrote in May, 2015.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Word vectors have become increasingly important in as features in machine learning models for natural language processing. The idea is to represent a word in some multidimensional space (the vector) in which words which are “similar” are similar in that space. Typically, one hopes that word vectors will provide some kind of semantic similarity. A typical party trick is to use word vectors to solve word analogy problems; e.g., king is to queen as man is to ? And hope that answer is woman.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deep Learning</title>
      <link>/posts/deep-learning/</link>
      <pubDate>Wed, 23 Apr 2025 09:34:01 -0400</pubDate>
      <guid>/posts/deep-learning/</guid>
      <description>&lt;p&gt;In this brief article, I will discuss an overview article from 2015 &amp;ndash; a long time ago in research years &amp;ndash; in &lt;em&gt;Nature&lt;/em&gt; called &lt;a href=&#34;https://www.nature.com/articles/nature14539&#34;&gt;&lt;em&gt;Deep Learning&lt;/em&gt;&lt;/a&gt;, essentially the underlying architecture of what is called machine learning these days. One of the co-authors is Geoffrey Hinton, who was awarded a Nobel Prize for his work over the decades on neural networks (Cool fact: Hinton is the great-great-grandson of George Boole, after whom boolean logic is named).&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Pythia and Ithaca tools</title>
      <link>/posts/pythia-ithica/</link>
      <pubDate>Fri, 11 Apr 2025 11:07:47 -0400</pubDate>
      <guid>/posts/pythia-ithica/</guid>
      <description>&lt;p&gt;The &lt;a href=&#34;https://doi.org/10.18653/v1/D19-1668&#34;&gt;Pythia&lt;/a&gt; and &lt;a href=&#34;https://doi.org/10.1038/s41586-022-04448-z&#34;&gt;Ithaca&lt;/a&gt; papers, by Assael et al., describe deep learning approaches to deciphering epigraphic data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapinal-Heras and Díaz-Sánchez, 2024: A Review of AI Applications in Human Sciences Research</title>
      <link>/posts/first-post/</link>
      <pubDate>Wed, 09 Apr 2025 00:00:00 +0000</pubDate>
      <guid>/posts/first-post/</guid>
      <description>&lt;p&gt;This overview article describes the use of AI in the &amp;ldquo;human sciences,&amp;rdquo; with a special emphasis on paleography, epigraphy, and archeology. It&amp;rsquo;s a good source for bibliography items, and we&amp;rsquo;ll look at several of the papers discussed in it. For our purposes, we&amp;rsquo;ll look especially at articles most connected to ancient languages. Here, we&amp;rsquo;ll just note some of the efforts, without a long discussion of each.&lt;/p&gt;&#xA;&lt;p&gt;Citation: Chapinal-Heras, Diego, and Carlos Díaz-Sánchez. 2024. “A Review of AI Applications in Human Sciences Research.” Digital Applications in Archaeology and Cultural Heritage 32 (March):e00323. &lt;a href=&#34;https://doi.org/10.1016/j.daach.2024.e00323&#34;&gt;https://doi.org/10.1016/j.daach.2024.e00323&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
