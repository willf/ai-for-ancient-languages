<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on AI For Ancient Languages</title>
    <link>/posts/</link>
    <description>Recent content in Posts on AI For Ancient Languages</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Will Fitzgerald</copyright>
    <lastBuildDate>Wed, 07 May 2025 09:10:10 -0400</lastBuildDate>
    <atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Comparing Full Text Search</title>
      <link>/posts/comparing-full-text-search/</link>
      <pubDate>Wed, 07 May 2025 09:10:10 -0400</pubDate>
      <guid>/posts/comparing-full-text-search/</guid>
      <description>&lt;h2 id=&#34;tlg-full-text-search&#34;&gt;TLG Full Text Search&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://stephanus.tlg.uci.edu/Iris/demo/tsearch.jsp&#34;&gt;Full text search&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Breadth of search:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Full corpus&lt;/li&gt;&#xA;&lt;li&gt;Works by Author&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;Flags:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Case sensitive&lt;/li&gt;&#xA;&lt;li&gt;Exact match&lt;/li&gt;&#xA;&lt;li&gt;Diacritic sensitive&lt;/li&gt;&#xA;&lt;li&gt;Adscript as subscript&lt;/li&gt;&#xA;&lt;li&gt;Wildcard&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;Proximity (within &lt;em&gt;n&lt;/em&gt; words)&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Near (before or after)&lt;/li&gt;&#xA;&lt;li&gt;Before&lt;/li&gt;&#xA;&lt;li&gt;After&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The proximate can be &lt;em&gt;word form&lt;/em&gt;, &lt;em&gt;lemma&lt;/em&gt;, or &lt;em&gt;grammatical category&lt;/em&gt;.&lt;/p&gt;&#xA;&lt;h3 id=&#34;search-guide&#34;&gt;Search Guide&lt;/h3&gt;&#xA;&lt;p&gt;The search language seems to be a subset of regular expressions; the definition is given by example.&lt;/p&gt;&#xA;&lt;p&gt;This is from the full text search guide:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Transformers (Way) Oversimplified</title>
      <link>/posts/transformers-oversimplified/</link>
      <pubDate>Thu, 01 May 2025 19:22:28 -0400</pubDate>
      <guid>/posts/transformers-oversimplified/</guid>
      <description>&lt;p&gt;The 2017 paper &lt;a href=&#34;https://arxiv.org/pdf/1706.03762&#34;&gt;&lt;em&gt;Attention is all you need&lt;/em&gt;&lt;/a&gt;, was revolutionary for machine learning for sequential data, in particular language data. In some ways, the main idea behind the paper is simple. Previous ML models processed each item in a sequence sequentially, with perhaps a small amount of context. This paper laid the groundwork for &lt;em&gt;transformer models&lt;/em&gt;. The abstract lays this out:&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;The dominant sequence transduction models are based on complex recurrent or&#xA;convolutional neural networks that include an encoder and a decoder. The best&#xA;performing models also connect the encoder and decoder through an attention&#xA;mechanism. We propose a new simple network architecture, the Transformer,&#xA;based solely on attention mechanisms, dispensing with recurrence and convolutions&#xA;entirely. Experiments on two machine translation tasks show these models to&#xA;be superior in quality while being more parallelizable and requiring significantly&#xA;less time to train. Our model achieves 28.4 BLEU&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; on the WMT 2014&#xA;English-to-German translation task, improving over the existing best results, including&#xA;ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,&#xA;our model establishes a new single-model state-of-the-art BLEU score of 41.8 after&#xA;training for 3.5 days on eight GPUs, a small fraction of the training costs of the&#xA;best models from the literature. We show that the Transformer generalizes well to&#xA;other tasks by applying it successfully to English constituency parsing both with&#xA;large and limited training data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Word Vectors Oversimplified</title>
      <link>/posts/word-vectors-oversimplified/</link>
      <pubDate>Wed, 23 Apr 2025 12:52:35 -0400</pubDate>
      <guid>/posts/word-vectors-oversimplified/</guid>
      <description>&lt;p&gt;&lt;em&gt;This is a &lt;a href=&#34;https://willwhim.wpengine.com/2015/05/12/word-vectors-machine-learning-oversimplified/&#34;&gt;reposting of an article&lt;/a&gt; I wrote in May, 2015.&lt;/em&gt;&lt;/p&gt;&#xA;&lt;p&gt;Word vectors have become increasingly important in as features in machine learning models for natural language processing. The idea is to represent a word in some multidimensional space (the vector) in which words which are “similar” are similar in that space. Typically, one hopes that word vectors will provide some kind of semantic similarity. A typical party trick is to use word vectors to solve word analogy problems; e.g., king is to queen as man is to ? And hope that answer is woman.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deep Learning</title>
      <link>/posts/deep-learning/</link>
      <pubDate>Wed, 23 Apr 2025 09:34:01 -0400</pubDate>
      <guid>/posts/deep-learning/</guid>
      <description>&lt;p&gt;In this brief article, I will discuss an overview article from 2015 &amp;ndash; a long time ago in research years &amp;ndash; in &lt;em&gt;Nature&lt;/em&gt; called &lt;a href=&#34;https://www.nature.com/articles/nature14539&#34;&gt;&lt;em&gt;Deep Learning&lt;/em&gt;&lt;/a&gt;, essentially the underlying architecture of what is called machine learning these days. One of the co-authors is Geoffrey Hinton, who was awarded a Nobel Prize for his work over the decades on neural networks (Cool fact: Hinton is the great-great-grandson of George Boole, after whom boolean logic is named).&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Pythia and Ithaca tools</title>
      <link>/posts/pythia-ithica/</link>
      <pubDate>Fri, 11 Apr 2025 11:07:47 -0400</pubDate>
      <guid>/posts/pythia-ithica/</guid>
      <description>&lt;p&gt;The &lt;a href=&#34;https://doi.org/10.18653/v1/D19-1668&#34;&gt;Pythia&lt;/a&gt; and &lt;a href=&#34;https://doi.org/10.1038/s41586-022-04448-z&#34;&gt;Ithaca&lt;/a&gt; papers, by Assael et al., describe deep learning approaches to deciphering epigraphic data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Chapinal-Heras and Díaz-Sánchez, 2024: A Review of AI Applications in Human Sciences Research</title>
      <link>/posts/first-post/</link>
      <pubDate>Wed, 09 Apr 2025 00:00:00 +0000</pubDate>
      <guid>/posts/first-post/</guid>
      <description>&lt;p&gt;This overview article describes the use of AI in the &amp;ldquo;human sciences,&amp;rdquo; with a special emphasis on paleography, epigraphy, and archeology. It&amp;rsquo;s a good source for bibliography items, and we&amp;rsquo;ll look at several of the papers discussed in it. For our purposes, we&amp;rsquo;ll look especially at articles most connected to ancient languages. Here, we&amp;rsquo;ll just note some of the efforts, without a long discussion of each.&lt;/p&gt;&#xA;&lt;p&gt;Citation: Chapinal-Heras, Diego, and Carlos Díaz-Sánchez. 2024. “A Review of AI Applications in Human Sciences Research.” Digital Applications in Archaeology and Cultural Heritage 32 (March):e00323. &lt;a href=&#34;https://doi.org/10.1016/j.daach.2024.e00323&#34;&gt;https://doi.org/10.1016/j.daach.2024.e00323&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
