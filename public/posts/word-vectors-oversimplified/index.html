<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Word Vectors Oversimplified | AI For Ancient Languages</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="This is a reposting of an article I wrote in May, 2015.
Word vectors have become increasingly important in as features in machine learning models for natural language processing. The idea is to represent a word in some multidimensional space (the vector) in which words which are “similar” are similar in that space. Typically, one hopes that word vectors will provide some kind of semantic similarity. A typical party trick is to use word vectors to solve word analogy problems; e.g., king is to queen as man is to ? And hope that answer is woman.">
    <meta name="generator" content="Hugo 0.145.0">
    
    
    
      <meta name="robots" content="index, follow">
    
    

    
<link rel="stylesheet" href="/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css" >




    


    
      

    

    

    
      <link rel="canonical" href="/posts/word-vectors-oversimplified/">
    

    <meta property="og:url" content="/posts/word-vectors-oversimplified/">
  <meta property="og:site_name" content="AI For Ancient Languages">
  <meta property="og:title" content="Word Vectors Oversimplified">
  <meta property="og:description" content="This is a reposting of an article I wrote in May, 2015.
Word vectors have become increasingly important in as features in machine learning models for natural language processing. The idea is to represent a word in some multidimensional space (the vector) in which words which are “similar” are similar in that space. Typically, one hopes that word vectors will provide some kind of semantic similarity. A typical party trick is to use word vectors to solve word analogy problems; e.g., king is to queen as man is to ? And hope that answer is woman.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-04-23T12:52:35-04:00">
    <meta property="article:modified_time" content="2025-04-23T12:52:35-04:00">

  <meta itemprop="name" content="Word Vectors Oversimplified">
  <meta itemprop="description" content="This is a reposting of an article I wrote in May, 2015.
Word vectors have become increasingly important in as features in machine learning models for natural language processing. The idea is to represent a word in some multidimensional space (the vector) in which words which are “similar” are similar in that space. Typically, one hopes that word vectors will provide some kind of semantic similarity. A typical party trick is to use word vectors to solve word analogy problems; e.g., king is to queen as man is to ? And hope that answer is woman.">
  <meta itemprop="datePublished" content="2025-04-23T12:52:35-04:00">
  <meta itemprop="dateModified" content="2025-04-23T12:52:35-04:00">
  <meta itemprop="wordCount" content="451">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Word Vectors Oversimplified">
  <meta name="twitter:description" content="This is a reposting of an article I wrote in May, 2015.
Word vectors have become increasingly important in as features in machine learning models for natural language processing. The idea is to represent a word in some multidimensional space (the vector) in which words which are “similar” are similar in that space. Typically, one hopes that word vectors will provide some kind of semantic similarity. A typical party trick is to use word vectors to solve word analogy problems; e.g., king is to queen as man is to ? And hope that answer is woman.">

      
    
	
  </head><body class="ma0 georgia bg-near-white production">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l center items-center justify-between">
    <a href="/" class="f3 fw2 hover-white white-90 dib no-underline">
      
        AI For Ancient Languages
      
    </a>
    <div class="flex-l items-center">
      

      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  
  <article class="flex-l mw8 center ph3 flex-wrap justify-between">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked ttu">
          
        Posts
      </aside><div id="sharing" class="mt3 ananke-socials"></div>
<h1 class="f1 athelas mt3 mb1">Word Vectors Oversimplified</h1>
      
      
      
      <time class="f6 mv4 dib tracked" datetime="2025-04-23T12:52:35-04:00">April 23, 2025</time>
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p><em>This is a <a href="https://willwhim.wpengine.com/2015/05/12/word-vectors-machine-learning-oversimplified/">reposting of an article</a> I wrote in May, 2015.</em></p>
<p>Word vectors have become increasingly important in as features in machine learning models for natural language processing. The idea is to represent a word in some multidimensional space (the vector) in which words which are “similar” are similar in that space. Typically, one hopes that word vectors will provide some kind of semantic similarity. A typical party trick is to use word vectors to solve word analogy problems; e.g., king is to queen as man is to ? And hope that answer is woman.</p>
<p>Word vectors are often created using distributional data. That is, words that co-occur tend to be more similar to words that don’t co-occur. So, a typical second step is to create a co-occurrence matrix of probabilities among words. Second step? Yes, because the first step (as usual) is to break things up into “words” (tokenization). Different tokenization schemes will create different models, of course, and there’s no particular reason to limit the tokens to dictionary words.</p>
<p>So, one way to think about a word vector of k feature weights is that the dot product of the word vectors of two words should be their co-occurrence probability. So this becomes the objective function to create a learned model; details are in the <a href="https://nlp.stanford.edu/pubs/glove.pdf">GloVe paper</a> (pdf file).</p>
<p>These models can be computationally expensive to create. Fortunately, Stanford and Google have provided word vector models based on a variety of corpora (and various hyperparameters, such as the number of dimensions). Stanford provides <a href="http://nlp.stanford.edu/projects/glove/">word vectors</a> trained on Wikipedia plus the <a href="https://catalog.ldc.upenn.edu/LDC2011T07">Gigaword</a> corpus, <a href="https://commoncrawl.org/">Common Crawl</a>, and two billion Twitter tweets at various dimension sizes. Google’s <a href="https://code.google.com/p/word2vec/">word2vec</a> system provides word vectors trained on 100 billion words from Google News, and anoother set created from Freebase entities.</p>
<h2 id="example">Example</h2>
<p>I’m from Michigan, and my wife and I often noted the similarity of Michigan to Wisconsin. The Detroit of Wisconsin is Milwaukee (largest, industrial cities); the Ann Arbor of Wisconsin is Madison (liberal, best college towns), and so on. But the Lansing of Wisconsin is also Madison (capital cities).</p>
<p>Radim Řehůřek created a fun little app<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> that uses the Google word2vect on the backend to do word analogies.</p>
<p>Here’s the word2vec results for the Michigan/Wisconsin comparisons:</p>
<ul>
<li>Michigan is to Detroit as Wisconsin is to Milwaukee. (yay!)</li>
<li>Michigan is to Ann Arbor as Wisconsin is to La Crosse. (boo!)</li>
<li>Michigan is to Lansing as Wisconsin is to La Crosse. (boo!)</li>
<li>Michigan is to Grand Rapids as Wisconsin is to La Crosse. (boo!)</li>
</ul>
<p>Ok, it seems to have a thing for La Cross. So, it’s not magic. But the fact that it’s producing somewhat similar cities (and not random words) is promising.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Alas, no longer working.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href="/" >
    &copy;  Will Fitzgerald 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>
