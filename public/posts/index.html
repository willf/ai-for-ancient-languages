<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>AI For Ancient Languages</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="">
    <meta name="generator" content="Hugo 0.145.0">
    
    
    
      <meta name="robots" content="index, follow">
    
    

    
<link rel="stylesheet" href="/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css" >




    


    
      

    

    
    
      <link href="/posts/index.xml" rel="alternate" type="application/rss+xml" title="AI For Ancient Languages" />
      <link href="/posts/index.xml" rel="feed" type="application/rss+xml" title="AI For Ancient Languages" />
      
    

    
      <link rel="canonical" href="/posts/">
    

    <meta property="og:url" content="/posts/">
  <meta property="og:site_name" content="AI For Ancient Languages">
  <meta property="og:title" content="Posts">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="website">

  <meta itemprop="name" content="Posts">
  <meta itemprop="datePublished" content="2025-05-07T09:10:10-04:00">
  <meta itemprop="dateModified" content="2025-05-07T09:10:10-04:00">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Posts">

      
    
	
  </head><body class="ma0 georgia bg-near-white production">

    

  <header>
    <div class="pb3-m pb6-l bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l center items-center justify-between">
    <a href="/" class="f3 fw2 hover-white white-90 dib no-underline">
      
        AI For Ancient Languages
      
    </a>
    <div class="flex-l items-center">
      

      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

      <div class="tc-l pv3 ph3 ph4-ns">
        <h1 class="f2 f-subheadline-l fw2 light-silver mb0 lh-title">
          Posts
        </h1>
        
      </div>
    </div>
  </header>


    <main class="pb7" role="main">
      
  <article class="pa3 pa4-ns nested-copy-line-height">
    <section class="cf ph3 ph5-l pv3 pv4-l f4 tc-l center measure-wide lh-copy nested-links mid-gray"></section>
    <section class="flex-ns mt5 flex-wrap justify-around">
      
        <div class="w-100 w-30-l mb4 relative bg-white">
          <div class="w-100 mb4 nested-copy-line-height relative bg-white">
  <div class="mb3 pa4 gray overflow-hidden bg-white">
    <span class="f6 db">Posts</span>
    <h1 class="f3 near-black">
      <a href="/posts/transformers-oversimplified/" class="link black dim">
        Transformers (Way) Oversimplified
      </a>
    </h1>
    <div class="nested-links f5 lh-copy nested-copy-line-height">
      <p>The 2017 paper <a href="https://arxiv.org/pdf/1706.03762"><em>Attention is all you need</em></a>, was revolutionary for machine learning for sequential data, in particular language data. In some ways, the main idea behind the paper is simple. Previous ML models processed each item in a sequence sequentially, with perhaps a small amount of context. This paper laid the groundwork for <em>transformer models</em>. The abstract lays this out:</p>
<blockquote>
<p>The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer,
based solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> on the WMT 2014
English-to-German translation task, improving over the existing best results, including
ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,
our model establishes a new single-model state-of-the-art BLEU score of 41.8 after
training for 3.5 days on eight GPUs, a small fraction of the training costs of the
best models from the literature. We show that the Transformer generalizes well to
other tasks by applying it successfully to English constituency parsing both with
large and limited training data.</p>
    </div>
    <a href="/posts/transformers-oversimplified/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
  </div>
</div>

        </div>
      
        <div class="w-100 w-30-l mb4 relative bg-white">
          <div class="w-100 mb4 nested-copy-line-height relative bg-white">
  <div class="mb3 pa4 gray overflow-hidden bg-white">
    <span class="f6 db">Posts</span>
    <h1 class="f3 near-black">
      <a href="/posts/word-vectors-oversimplified/" class="link black dim">
        Word Vectors Oversimplified
      </a>
    </h1>
    <div class="nested-links f5 lh-copy nested-copy-line-height">
      <p><em>This is a <a href="https://willwhim.wpengine.com/2015/05/12/word-vectors-machine-learning-oversimplified/">reposting of an article</a> I wrote in May, 2015.</em></p>
<p>Word vectors have become increasingly important in as features in machine learning models for natural language processing. The idea is to represent a word in some multidimensional space (the vector) in which words which are “similar” are similar in that space. Typically, one hopes that word vectors will provide some kind of semantic similarity. A typical party trick is to use word vectors to solve word analogy problems; e.g., king is to queen as man is to ? And hope that answer is woman.</p>
    </div>
    <a href="/posts/word-vectors-oversimplified/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
  </div>
</div>

        </div>
      
        <div class="w-100 w-30-l mb4 relative bg-white">
          <div class="w-100 mb4 nested-copy-line-height relative bg-white">
  <div class="mb3 pa4 gray overflow-hidden bg-white">
    <span class="f6 db">Posts</span>
    <h1 class="f3 near-black">
      <a href="/posts/deep-learning/" class="link black dim">
        Deep Learning
      </a>
    </h1>
    <div class="nested-links f5 lh-copy nested-copy-line-height">
      <p>In this brief article, I will discuss an overview article from 2015 &ndash; a long time ago in research years &ndash; in <em>Nature</em> called <a href="https://www.nature.com/articles/nature14539"><em>Deep Learning</em></a>, essentially the underlying architecture of what is called machine learning these days. One of the co-authors is Geoffrey Hinton, who was awarded a Nobel Prize for his work over the decades on neural networks (Cool fact: Hinton is the great-great-grandson of George Boole, after whom boolean logic is named).</p>
    </div>
    <a href="/posts/deep-learning/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
  </div>
</div>

        </div>
      
        <div class="w-100 w-30-l mb4 relative bg-white">
          <div class="w-100 mb4 nested-copy-line-height relative bg-white">
  <div class="mb3 pa4 gray overflow-hidden bg-white">
    <span class="f6 db">Posts</span>
    <h1 class="f3 near-black">
      <a href="/posts/pythia-ithica/" class="link black dim">
        The Pythia and Ithaca tools
      </a>
    </h1>
    <div class="nested-links f5 lh-copy nested-copy-line-height">
      <p>The <a href="https://doi.org/10.18653/v1/D19-1668">Pythia</a> and <a href="https://doi.org/10.1038/s41586-022-04448-z">Ithaca</a> papers, by Assael et al., describe deep learning approaches to deciphering epigraphic data.</p>
    </div>
    <a href="/posts/pythia-ithica/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
  </div>
</div>

        </div>
      
        <div class="w-100 w-30-l mb4 relative bg-white">
          <div class="w-100 mb4 nested-copy-line-height relative bg-white">
  <div class="mb3 pa4 gray overflow-hidden bg-white">
    <span class="f6 db">Posts</span>
    <h1 class="f3 near-black">
      <a href="/posts/first-post/" class="link black dim">
        Chapinal-Heras and Díaz-Sánchez, 2024: A Review of AI Applications in Human Sciences Research
      </a>
    </h1>
    <div class="nested-links f5 lh-copy nested-copy-line-height">
      <p>This overview article describes the use of AI in the &ldquo;human sciences,&rdquo; with a special emphasis on paleography, epigraphy, and archeology. It&rsquo;s a good source for bibliography items, and we&rsquo;ll look at several of the papers discussed in it. For our purposes, we&rsquo;ll look especially at articles most connected to ancient languages. Here, we&rsquo;ll just note some of the efforts, without a long discussion of each.</p>
<p>Citation: Chapinal-Heras, Diego, and Carlos Díaz-Sánchez. 2024. “A Review of AI Applications in Human Sciences Research.” Digital Applications in Archaeology and Cultural Heritage 32 (March):e00323. <a href="https://doi.org/10.1016/j.daach.2024.e00323">https://doi.org/10.1016/j.daach.2024.e00323</a>.</p>
    </div>
    <a href="/posts/first-post/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
  </div>
</div>

        </div>
      
    </section></article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href="/" >
    &copy;  Will Fitzgerald 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>
