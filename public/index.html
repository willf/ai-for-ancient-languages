<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>AI For Ancient Languages</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="">
    <meta name="generator" content="Hugo 0.145.0">
    
    
    
      <meta name="robots" content="index, follow">
    
    

    
<link rel="stylesheet" href="/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css" >




    


    
      

    

    
    
      <link href="/index.xml" rel="alternate" type="application/rss+xml" title="AI For Ancient Languages" />
      <link href="/index.xml" rel="feed" type="application/rss+xml" title="AI For Ancient Languages" />
      
    

    
      <link rel="canonical" href="/">
    

    <meta property="og:url" content="/">
  <meta property="og:site_name" content="AI For Ancient Languages">
  <meta property="og:title" content="AI For Ancient Languages">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="website">

  <meta itemprop="name" content="AI For Ancient Languages">
  <meta itemprop="datePublished" content="2025-05-07T09:10:10-04:00">
  <meta itemprop="dateModified" content="2025-05-07T09:10:10-04:00">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="AI For Ancient Languages">

      
    
	
  </head><body class="ma0 georgia bg-near-white production">

    

  <header>
    <div class="pb3-m pb6-l bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l center items-center justify-between">
    <a href="/" class="f3 fw2 hover-white white-90 dib no-underline">
      
        AI For Ancient Languages
      
    </a>
    <div class="flex-l items-center">
      

      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

      <div class="tc-l pv3 ph3 ph4-ns">
        <h1 class="f2 f-subheadline-l fw2 light-silver mb0 lh-title">
          AI For Ancient Languages
        </h1>
        
      </div>
    </div>
  </header>


    <main class="pb7" role="main">
      
  <article class="cf ph3 ph5-l pv3 pv4-l f4 tc-l center measure-wide lh-copy nested-links mid-gray">
    
  </article>

  
  
  
  
  

  
    <div class="pa3 pa4-ns w-100 w-70-ns center">
      

      <section class="w-100 mw8">
        
        
          <div class="w-100 mb4 relative">
            
<article class="bb b--black-10">
  <div class="db pv4 ph3 ph0-l dark-gray no-underline">
    <div class="flex-column flex-row-ns flex">
      
      <div class="blah w-100">
        <h1 class="f3 fw1 athelas mt0 lh-title">
          <a href="/posts/comparing-full-text-search/" class="color-inherit dim link">
            Comparing Full Text Search
            </a>
        </h1>
        <div class="f6 f5-l lh-copy nested-copy-line-height nested-links">
          <h2 id="tlg-full-text-search">TLG Full Text Search</h2>
<p><a href="https://stephanus.tlg.uci.edu/Iris/demo/tsearch.jsp">Full text search</a></p>
<p>Breadth of search:</p>
<ol>
<li>Full corpus</li>
<li>Works by Author</li>
</ol>
<p>Flags:</p>
<ol>
<li>Case sensitive</li>
<li>Exact match</li>
<li>Diacritic sensitive</li>
<li>Adscript as subscript</li>
<li>Wildcard</li>
</ol>
<p>Proximity (within <em>n</em> words)</p>
<ul>
<li>Near (before or after)</li>
<li>Before</li>
<li>After</li>
</ul>
<p>The proximate can be <em>word form</em>, <em>lemma</em>, or <em>grammatical category</em>.</p>
<h3 id="search-guide">Search Guide</h3>
<p>The search language seems to be a subset of regular expressions; the definition is given by example.</p>
<p>This is from the full text search guide:</p>
        </div>
        <a href="/posts/comparing-full-text-search/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
      </div>
    </div>
  </div>
</article>

          </div>
        
          <div class="w-100 mb4 relative">
            
<article class="bb b--black-10">
  <div class="db pv4 ph3 ph0-l dark-gray no-underline">
    <div class="flex-column flex-row-ns flex">
      
      <div class="blah w-100">
        <h1 class="f3 fw1 athelas mt0 lh-title">
          <a href="/posts/transformers-oversimplified/" class="color-inherit dim link">
            Transformers (Way) Oversimplified
            </a>
        </h1>
        <div class="f6 f5-l lh-copy nested-copy-line-height nested-links">
          <p>The 2017 paper <a href="https://arxiv.org/pdf/1706.03762"><em>Attention is all you need</em></a>, was revolutionary for machine learning for sequential data, in particular language data. In some ways, the main idea behind the paper is simple. Previous ML models processed each item in a sequence sequentially, with perhaps a small amount of context. This paper laid the groundwork for <em>transformer models</em>. The abstract lays this out:</p>
<blockquote>
<p>The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer,
based solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> on the WMT 2014
English-to-German translation task, improving over the existing best results, including
ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,
our model establishes a new single-model state-of-the-art BLEU score of 41.8 after
training for 3.5 days on eight GPUs, a small fraction of the training costs of the
best models from the literature. We show that the Transformer generalizes well to
other tasks by applying it successfully to English constituency parsing both with
large and limited training data.</p>
        </div>
        <a href="/posts/transformers-oversimplified/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
      </div>
    </div>
  </div>
</article>

          </div>
        
          <div class="w-100 mb4 relative">
            
<article class="bb b--black-10">
  <div class="db pv4 ph3 ph0-l dark-gray no-underline">
    <div class="flex-column flex-row-ns flex">
      
      <div class="blah w-100">
        <h1 class="f3 fw1 athelas mt0 lh-title">
          <a href="/posts/word-vectors-oversimplified/" class="color-inherit dim link">
            Word Vectors Oversimplified
            </a>
        </h1>
        <div class="f6 f5-l lh-copy nested-copy-line-height nested-links">
          <p><em>This is a <a href="https://willwhim.wpengine.com/2015/05/12/word-vectors-machine-learning-oversimplified/">reposting of an article</a> I wrote in May, 2015.</em></p>
<p>Word vectors have become increasingly important in as features in machine learning models for natural language processing. The idea is to represent a word in some multidimensional space (the vector) in which words which are “similar” are similar in that space. Typically, one hopes that word vectors will provide some kind of semantic similarity. A typical party trick is to use word vectors to solve word analogy problems; e.g., king is to queen as man is to ? And hope that answer is woman.</p>
        </div>
        <a href="/posts/word-vectors-oversimplified/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a>
      </div>
    </div>
  </div>
</article>

          </div>
        
      </section>

      
        <section class="w-100">
          <h1 class="f3">More</h1>
          
          
            <h2 class="f5 fw4 mb4 dib mr3">
              <a href="/posts/deep-learning/" class="link black dim">
                Deep Learning
              </a>
            </h2>
          
            <h2 class="f5 fw4 mb4 dib mr3">
              <a href="/posts/pythia-ithica/" class="link black dim">
                The Pythia and Ithaca tools
              </a>
            </h2>
          
            <h2 class="f5 fw4 mb4 dib mr3">
              <a href="/posts/first-post/" class="link black dim">
                Chapinal-Heras and Díaz-Sánchez, 2024: A Review of AI Applications in Human Sciences Research
              </a>
            </h2>
          
        </section>
      

    </div>
  

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href="/" >
    &copy;  Will Fitzgerald 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>
